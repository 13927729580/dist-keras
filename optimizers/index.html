<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Joeri Hermans">
  
  <title>Optimizers - Distributed Keras</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Optimizers";
    var mkdocs_page_input_path = "optimizers.md";
    var mkdocs_page_url = "/optimizers/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Distributed Keras</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Optimizers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#optimizers">Optimizers</a></li>
                
                    <li><a class="toctree-l4" href="#single-trainer">Single Trainer</a></li>
                
                    <li><a class="toctree-l4" href="#easgd">EASGD</a></li>
                
                    <li><a class="toctree-l4" href="#asynchronous-easgd">Asynchronous EASGD</a></li>
                
                    <li><a class="toctree-l4" href="#asynchronous-eamsgd">Asynchronous EAMSGD</a></li>
                
                    <li><a class="toctree-l4" href="#downpour">DOWNPOUR</a></li>
                
                    <li><a class="toctree-l4" href="#custom-distributed-optimizer">Custom distributed optimizer</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Distributed Keras</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Optimizers</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/JoeriHermans/dist-keras" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="optimizers">Optimizers</h1>
<p>Optimizers, or trainers, are the main component in Distributed Keras (DK). All trainers share a single interface, which is the <code>Trainer</code> class, defined in <code>distkeras/distributed.py</code>. This class also contains the <code>serialized model</code>, the <code>loss</code>, and the <code>Keras optimizer</code> the workers need to use. Generally, a trainer will run on a single worker. In the context of Apache Spark, this means that the thread which is responsible for doing the foreachPartition or mapPartitions will have been assigned a trainer. In reality however, the training of the model itself will utilise more physical cores. In fact, it will employ all available cores, and thus bypassing resource managers such as YARN.</p>
<h2 id="single-trainer">Single Trainer</h2>
<p>A single trainer is in all simplicity a trainer which will use a single thread (as discussed above) to train a model. This trainer is usually used as a baseline metric for new distributed optimizers.</p>
<pre><code class="python">SingleTrainer(keras_model, worker_optimizer, loss, num_epoch=1,
              batch_size=32, features_col=&quot;features&quot;, label_col=&quot;label&quot;)
</code></pre>

<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>keras_model</strong>:            The Keras model which should be trained.</li>
<li><strong>worker_optmizer</strong>:        Keras optimizer for workers.</li>
<li><strong>num_epoch</strong>:              Number of epoch iterations over the data.</li>
<li><strong>batch_size</strong>:             Mini-batch size.</li>
<li><strong>features_col</strong>:           Column of the feature vector in the Spark Dataframe.</li>
<li><strong>label_col</strong>:              Column of the label in the Spark Dataframe.</li>
</ul>
<h2 id="easgd">EASGD</h2>
<p>The distinctive idea of EASGD is to allow the local workers to perform more exploration (small rho) and the master to perform exploitation. This approach differs from other settings explored in the literature, and focus on how fast the center variable converges <a href="https://arxiv.org/pdf/1412.6651.pdf">(paper)</a> .</p>
<p>We want to note the basic version of EASGD is a synchronous algorithm, i.e., once a worker is done processing a batch of the data, it will wait until all other workers have submitted their variables (this includes the weight parameterization, iteration number, and worker id) to the parameter server before starting the next data batch.</p>
<pre><code class="python">EASGD(keras_model, worker_optimizer, loss, num_workers=2, features_col=&quot;features&quot;, label_col=&quot;label&quot;,
      rho=5.0, learning_rate=0.01, batch_size=32, num_epoch=1, master_port=5000)
</code></pre>

<p><strong>Parameters</strong>:</p>
<p>TODO</p>
<h2 id="asynchronous-easgd">Asynchronous EASGD</h2>
<p>In this section we propose the asynchronous version of EASGD. Instead of waiting on the synchronization of other trainers, this method communicates the elastic difference (as described in the paper), with the parameter server. The only synchronization mechanism that has been implemented, is to ensure no race-conditions occur when updating the center variable.</p>
<pre><code class="python">AsynchronousEASGD(keras_model, worker_optimizer, loss, num_workers=2, batch_size=1000,
                  features_col=&quot;features&quot;, label_col=&quot;label&quot;, communication_window=3,
                  rho=0.01, learning_rate=0.01, master_port=5000, num_epoch=1)
</code></pre>

<p><strong>Parameters</strong>:</p>
<p>TODO</p>
<h2 id="asynchronous-eamsgd">Asynchronous EAMSGD</h2>
<p>Asynchronous EAMSGD is a variant of asynchronous EASGD. It is based on the Nesterov's momentum scheme, where the update of the local worker is modified to incorepare a momentum term.</p>
<pre><code class="python">AsynchornousEAMSGD(keras_model, worker_optimizer, loss, num_workers=2, batch_size=32,
                  features_col=&quot;features&quot;, label_col=&quot;label&quot;, communication_window=10,
                  rho=5.0, learning_rate=0.01, momentum=0.9, master_port=5000, num_epoch=1)
</code></pre>

<p><strong>Parameters</strong>:</p>
<p>TODO</p>
<h2 id="downpour">DOWNPOUR</h2>
<p>An asynchronous stochastic gradient descent procedure supporting a large number of model replicas and leverages adaptive learning rates. This implementation is based on the pseudocode provided by <a href="https://arxiv.org/pdf/1412.6651.pdf">Zhang et al.</a></p>
<pre><code class="python">DOWNPOUR(keras_model, worker_optimizer, loss, num_workers=2, batch_size=1000,
         features_col=&quot;features&quot;, label_col=&quot;label&quot;, communication_window=5,
         master_port=5000, num_epoch=1, learning_rate=0.01))
</code></pre>

<p><strong>Parameters</strong>:</p>
<p>TODO</p>
<h2 id="custom-distributed-optimizer">Custom distributed optimizer</h2>
<p>TODO</p>
<h3 id="synchronized-distributed-trainer">Synchronized Distributed Trainer</h3>
<p>TODO</p>
<h3 id="asynchronous-distributed-trainer">Asynchronous Distributed Trainer</h3>
<p>TODO</p>
<h3 id="implementing-a-custom-worker">Implementing a custom worker</h3>
<p>TODO</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../license/" class="btn btn-neutral float-right" title="License">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
      <p>Copyright (c) 2016 Joeri Hermans</p>
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/JoeriHermans/dist-keras" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../license/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
