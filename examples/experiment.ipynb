{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Experiment\n",
    "\n",
    "In this notebook we will evaluate all distributed optimizers and compare their performance. Furthermore, you can extend this notebook with your own algorithms in order to have a baseline metric and proof for future pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from distkeras.distributed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Experimentation\"\n",
    "master = \"local[*]\"\n",
    "using_spark_2 = False\n",
    "num_cores = 3\n",
    "num_executors = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This variable is derived from the number of cores and executors, and will \n",
    "# be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a Spark Context\n",
    "\n",
    "In order to read our (big) dataset into our Spark Cluster, we first need a Spark Context. However, since Spark 2.0 there are some changes in the initialization of a Spark Context. For example, SQLContext and HiveContext do not have to be initialized seperatly anymore, i.e., the initialization process is simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double-check the inferred schema.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing and normalization\n",
    "\n",
    "Since Spark's MLlib has some nice features for some distributed dataprocessing, we follow MLlib (dataframe) API in order to ensure compatibility. What it basically boils down to, is that all the features (which can have different type) will be aggregated into a single column. More information on Spark MLlib (and other APIs) can be found here: [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "In the following steps we will show you how to extract the desired columns from the dataset and prepare the for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\"\n",
    "# which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index\n",
    "# so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some properties of the neural network for later use.\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We observe that Keras is not able to work with these indexes. What it actually\n",
    "# expects is a vector with an identical size to the output layer. Our framework provides\n",
    "# functionality to do this with ease. What it basically does, given an expected vector dimension,\n",
    "# it prepares zero vector with the specified dimensionality, and will set the neuron with a specific\n",
    "# label index to one.\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "\n",
    "label_vector_transformer = LabelVectorTransformer(output_dim=nb_classes)\n",
    "dataset = label_vector_transformer.transform(dataset).toDF()\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(trainingSet, testSet) = dataset.randomSplit([0.7, 0.3])\n",
    "trainingSet.cache()\n",
    "testSet.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "\n",
    "We will now construct a relatively simple Keras model (without any modifications) which, hopefully, will be able to classify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(600))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarize the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the following cells we will train and evaluate the model using different distributed trainers, however, we will as well provide a baseline metric using a **SingleTrainer**, which is basically an instance of the Adagrad optimizer running on Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SingleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_trainer = SingleTrainer(keras_model=model, features_col=\"features_normalized\", batch_size=10000)\n",
    "trained_model = single_trainer.train(trainingSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EASGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous EASGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNPOUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
