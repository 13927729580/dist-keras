{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "**Joeri Hermans** (Technical Student, IT-DB-SAS, CERN)             \n",
    "*Departement of Knowledge Engineering*         \n",
    "*Maastricht University, The Netherlands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be analyzing the a relatively large preprocessed dataset, and apply some deep learning models to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from distkeras.distributed import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Deep Learning: Analysis\"\n",
    "using_spark_2 = False\n",
    "yarn = \"p01001532067275.cern.ch:8088\" # Address:port of ResourceManager\n",
    "if not yarn:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    max_num_executors = 11\n",
    "    num_cores = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if YARN is specified.\n",
    "if yarn:\n",
    "    # Build the ResourceManager metrics URI.\n",
    "    yarn_metrics_uri = \"http://\" + yarn + \"/ws/v1/cluster/metrics\"\n",
    "    # Fetch the number of available nodes\n",
    "    response = requests.get(yarn_metrics_uri)\n",
    "    data = response.json()\n",
    "    # Fetch the number of active nodes.\n",
    "    num_active_nodes = int(data['clusterMetrics']['activeNodes'])\n",
    "    # Assign the number of executors.\n",
    "    num_executors = num_active_nodes\n",
    "    if num_executors > max_num_executors:\n",
    "        num_executors = max_num_executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 11\n",
      "Number of desired cores / executor: 2\n",
      "Total number of workers: 22\n"
     ]
    }
   ],
   "source": [
    "# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired cores / executor: \" + `num_cores`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.parquet(\"data/processed_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features_normalized: vector (nullable = true)\n",
      " |-- label_index: double (nullable = true)\n",
      " |-- label: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the schema.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 30\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "nb_features = len(raw_dataset.select(\"features_normalized\").take(1)[0][\"features_normalized\"])\n",
    "nb_classes = len(raw_dataset.select(\"label\").take(1)[0][\"label\"])\n",
    "\n",
    "print(\"Number of features: \" + str(nb_features))\n",
    "print(\"Number of classes: \" + str(nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features_normalized: vector, label_index: double, label: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(trainingSet, testSet) = raw_dataset.randomSplit([0.7, 0.3])\n",
    "trainingSet.cache()\n",
    "testSet.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.4))\n",
    "#model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 500)           15500       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 500)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 500)           250500      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 500)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             1002        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 2)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 267002\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_name = \"f1\"\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=metric_name, predictionCol=\"predicted_index\", labelCol=\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    global testSet\n",
    "    \n",
    "    # Clear the prediction column from the testset.\n",
    "    testSet = testSet.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    testSet = predictor.predict(testSet)\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    testSet = index_transformer.transform(testSet)\n",
    "    # Store the F1 score of the SingleTrainer.\n",
    "    score = evaluator.evaluate(testSet)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_transformer = LabelIndexTransformer(output_dim=nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "time_spent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribute the training and test set to the workers.\n",
    "testSet = testSet.repartition(num_workers)\n",
    "trainingSet = trainingSet.repartition(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testset instances: 75236\n",
      "Number of trainingset instances: 174764\n",
      "Total number of instances: 250000\n"
     ]
    }
   ],
   "source": [
    "# Count the instances.\n",
    "num_test_set = testSet.count()\n",
    "num_training_set = trainingSet.count()\n",
    "\n",
    "print(\"Number of testset instances: \" + str(num_test_set))\n",
    "print(\"Number of trainingset instances: \" + str(num_training_set))\n",
    "print(\"Total number of instances: \" + str(num_test_set + num_training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent (Asynchronous EASGD): 17.88398814201355 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "async_easgd_trainer = AsynchronousEASGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                        batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                        worker_optimizer=optimizer, loss=loss, communication_window=30)\n",
    "trained_model = async_easgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['async_easgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (Asynchronous EASGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (EASGD): 0.8023815453380814\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['easgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (EASGD): \" + `score`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
