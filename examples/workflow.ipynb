{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Keras Experiment\n",
    "\n",
    "In this notebook we will evaluate all distributed optimizers and compare their performance. Furthermore, you can extend this notebook with your own algorithms in order to have a baseline metric and proof for future pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from distkeras.distributed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Experimentation\"\n",
    "using_spark_2 = False\n",
    "yarn = None # Address:port of ResourceManager\n",
    "if not yarn:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    # Otherwise set the number of maximum executors and\n",
    "    # cores per executor.\n",
    "    max_num_executors = 20\n",
    "    num_cores = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if YARN is specified.\n",
    "if yarn:\n",
    "    # Build the ResourceManager metrics URI.\n",
    "    yarn_metrics_uri = \"http://\" + yarn + \"/ws/v1/cluster/metrics\"\n",
    "    # Fetch the number of available nodes\n",
    "    response = requests.get(yarn_metrics_uri)\n",
    "    data = response.json()\n",
    "    # Fetch the number of active nodes.\n",
    "    num_active_nodes = int(data['clusterMetrics']['activeNodes'])\n",
    "    # Assign the number of executors.\n",
    "    num_executors = num_active_nodes\n",
    "    if max_num_executors > num_executors:\n",
    "        num_executors = max_num_executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This variable is derived from the number of cores and executors, and will \n",
    "# be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a Spark Context\n",
    "\n",
    "In order to read our (big) dataset into our Spark Cluster, we first need a Spark Context. However, since Spark 2.0 there are some changes in the initialization of a Spark Context. For example, SQLContext and HiveContext do not have to be initialized seperatly anymore, i.e., the initialization process is simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EventId: integer (nullable = true)\n",
      " |-- DER_mass_MMC: double (nullable = true)\n",
      " |-- DER_mass_transverse_met_lep: double (nullable = true)\n",
      " |-- DER_mass_vis: double (nullable = true)\n",
      " |-- DER_pt_h: double (nullable = true)\n",
      " |-- DER_deltaeta_jet_jet: double (nullable = true)\n",
      " |-- DER_mass_jet_jet: double (nullable = true)\n",
      " |-- DER_prodeta_jet_jet: double (nullable = true)\n",
      " |-- DER_deltar_tau_lep: double (nullable = true)\n",
      " |-- DER_pt_tot: double (nullable = true)\n",
      " |-- DER_sum_pt: double (nullable = true)\n",
      " |-- DER_pt_ratio_lep_tau: double (nullable = true)\n",
      " |-- DER_met_phi_centrality: double (nullable = true)\n",
      " |-- DER_lep_eta_centrality: double (nullable = true)\n",
      " |-- PRI_tau_pt: double (nullable = true)\n",
      " |-- PRI_tau_eta: double (nullable = true)\n",
      " |-- PRI_tau_phi: double (nullable = true)\n",
      " |-- PRI_lep_pt: double (nullable = true)\n",
      " |-- PRI_lep_eta: double (nullable = true)\n",
      " |-- PRI_lep_phi: double (nullable = true)\n",
      " |-- PRI_met: double (nullable = true)\n",
      " |-- PRI_met_phi: double (nullable = true)\n",
      " |-- PRI_met_sumet: double (nullable = true)\n",
      " |-- PRI_jet_num: integer (nullable = true)\n",
      " |-- PRI_jet_leading_pt: double (nullable = true)\n",
      " |-- PRI_jet_leading_eta: double (nullable = true)\n",
      " |-- PRI_jet_leading_phi: double (nullable = true)\n",
      " |-- PRI_jet_subleading_pt: double (nullable = true)\n",
      " |-- PRI_jet_subleading_eta: double (nullable = true)\n",
      " |-- PRI_jet_subleading_phi: double (nullable = true)\n",
      " |-- PRI_jet_all_pt: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Double-check the inferred schema.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing and normalization\n",
    "\n",
    "Since Spark's MLlib has some nice features for some distributed dataprocessing, we follow MLlib (dataframe) API in order to ensure compatibility. What it basically boils down to, is that all the features (which can have different type) will be aggregated into a single column. More information on Spark MLlib (and other APIs) can be found here: [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "In the following steps we will show you how to extract the desired columns from the dataset and prepare the for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\"\n",
    "# which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index\n",
    "# so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some properties of the neural network for later use.\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We observe that Keras is not able to work with these indexes. What it actually\n",
    "# expects is a vector with an identical size to the output layer. Our framework provides\n",
    "# functionality to do this with ease. What it basically does, given an expected vector dimension,\n",
    "# it prepares zero vector with the specified dimensionality, and will set the neuron with a specific\n",
    "# label index to one.\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "\n",
    "label_vector_transformer = LabelVectorTransformer(output_dim=nb_classes, input_col=\"label_index\", output_col=\"label\")\n",
    "dataset = label_vector_transformer.transform(dataset).toDF()\n",
    "# Only select the columns we need (less data shuffling) while training.\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features_normalized: vector, label_index: double, label: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(trainingSet, testSet) = dataset.randomSplit([0.7, 0.3])\n",
    "trainingSet.cache()\n",
    "testSet.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "\n",
    "We will now construct a relatively simple Keras model (without any modifications) which, hopefully, will be able to classify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(600))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 600)           18600       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 600)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 600)           0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 600)           360600      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 600)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             1202        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 2)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 380402\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Optimizer and Loss\n",
    "\n",
    "In order to evaluate the gradient on the model replicas, we have to specify an optimizer and a loss method. For this, we just follow the Keras API as defined in the documentation: [https://keras.io/optimizers/](https://keras.io/optimizers/) and [https://keras.io/objectives/](https://keras.io/objectives/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop()\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the following cells we will train and evaluate the model using different distributed trainers, however, we will as well provide a baseline metric using a **SingleTrainer**, which is basically an instance of the Adagrad optimizer running on Spark.\n",
    "\n",
    "Furthermore, we will also evaluate every training using Spark's MulticlassClassificationEvaluator [https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "\n",
    "We will evaluate all algorithms using the F1 [https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_name = \"f1\"\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=metric_name, predictionCol=\"predicted_index\", labelCol=\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    global testSet\n",
    "    \n",
    "    # Clear the prediction column from the testset.\n",
    "    testSet = testSet.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    testSet = predictor.predict(testSet).toDF()\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    testSet = index_transformer.transform(testSet).toDF()\n",
    "    # Store the F1 score of the SingleTrainer.\n",
    "    score = evaluator.evaluate(testSet)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we can evaluate the prediction, we will also need to converted the neural network prediction, which is a vector which has the same dimensionality as the output layer, to a label index. This is shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_transformer = LabelIndexTransformer(output_dim=nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to keep track of the evaluated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "time_spent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repartition the testSet to the number of parallel trainers.\n",
    "testSet = testSet.repartition(num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SingleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "single_trainer = SingleTrainer(keras_model=model, loss=loss, worker_optimizer=optimizer, \n",
    "                               features_col=\"features_normalized\", batch_size=1000)\n",
    "trained_model = single_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['single_trainer'] = dt\n",
    "\n",
    "# Note that this time also includes shuffling the data from different places. If you run this\n",
    "# for the second time (after each other), it will be a lot faster since the data is already\n",
    "# on the physical machine.\n",
    "print(\"Time spent (SingleTrainer): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['single_trainer'] = score\n",
    "\n",
    "print(\"F1 (SingleTrainer): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "easgd_trainer = EASGD(keras_model=model, features_col=\"features_normalized\", batch_size=500,\n",
    "                      num_workers=num_workers, rho=5.0, learning_rate=0.2)\n",
    "trained_model = easgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['easgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (EASGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['easgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (EASGD): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous EASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "async_easgd_trainer = AsynchronousEASGD(keras_model=model, features_col=\"features_normalized\", batch_size=500,\n",
    "                                        num_workers=num_workers, rho=5.0, learning_rate=0.2)\n",
    "trained_model = async_easgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['async_easgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (Asynchronous EASGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['async_easgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (Asynchronous EASGD): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNPOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "downpour_trainer = DOWNPOUR(keras_model=model, features_col=\"features_normalized\", batch_size=10,\n",
    "                   num_workers=num_workers, learning_rate=0.2)\n",
    "trained_model = downpour_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['downpour_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (DOWNPOUR): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['downpour_trainer'] = score\n",
    "\n",
    "print(\"F1 (DOWNPOUR): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of initial experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the F1 score of the trainers.\n",
    "plt.bar(range(len(results)), results.values(), 0.2, align='center', color='b')\n",
    "plt.xticks(range(len(results)), results.keys(), rotation=25)\n",
    "plt.xlabel('Trainers')\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"F1 score with different trainers - (higher is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training time of the trainers.\n",
    "plt.bar(range(len(time_spent)), time_spent.values(), 0.2, align='center', color='b')\n",
    "plt.xticks(range(len(time_spent)), time_spent.keys(), rotation=25)\n",
    "plt.xlabel('Trainers')\n",
    "plt.ylabel('Seconds')\n",
    "plt.title(\"Training time - (lower is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of parallel workers: \" + `num_workers`)\n",
    "improvement = (time_spent['single_trainer'] / time_spent['async_easgd_trainer']) * 100.0\n",
    "print(\"Improvement using AEASGD: \" + `improvement` + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Of course, this is not a representative evaluation since distributed algorithms really benefit when the number of parallel trainers is a lot higher. In order to evaluate correctly, we must set up an experiment which will be able to make a clear picture. First, we will include training time, and run the experiments multiple times using a different number of workers. Then, we will also evaluate the different batch_sizes, since this will influence the communication periods, and gradient residuals. Furthermore, we will also examine the learning rate.\n",
    "\n",
    "**Note 1**: It would be nice to obtain the aggregated loss of all the trainers. However, this is not *yet* possible in Distributed Keras.\n",
    "\n",
    "**Note 2**: In this experiment we will iterate over the number of **POSSIBLE** parallel trainers. If your cluster allows for more CPU cores to be used, you will obtain different results compared to a system with less CPU cores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
