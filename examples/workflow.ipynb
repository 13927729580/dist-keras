{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Deep Learning with Apache Spark and Keras\n",
    "\n",
    "**Joeri Hermans** (Technical Student, IT-DB-SAS, CERN)             \n",
    "*Departement of Knowledge Engineering*         \n",
    "*Maastricht University, The Netherlands*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!(date +%d\\ %B\\ %G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This presentation will give the reader an introduction to the topic of distributed deep learning (DDL) and to the issues which need to be taken into consideration when applying this technique. We will also introduce a DDL framework based on a **fast and general engine for large-scale data processing** called [Apache Spark](https://spark.apache.org/) and the **neural network library** [Keras](https://keras.io). \n",
    "\n",
    "The project was initially initiated by the CMS experiment. CMS is exploring the possibility to use a deep learning model for the high level trigger in order to be able to handle the data rates for LHC run 3 and up. Furthermore, they would like to be able to train their models faster using distributed algorithms which allows them to tune their models with an increased frequency. An other requirement was those models should be trained on their complete dataset, which is in the order of a TB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction and problem statement](#Distributed-Deep-Learning,-an-introduction.)\n",
    "  - [Model parallelism](#Model-parallelism)\n",
    "  - [Data parallelism](#Data-parallelism)\n",
    "- [Usage](#Distributed-Keras:-a-practicle-example)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Scaling](#Scaling)\n",
    "- [Experimental observations](#Experimental-observations)\n",
    "- [Summary](#Summary)\n",
    "- [Future work](#Future-work)\n",
    "- [Acknowledgments](#Acknowledgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Deep Learning, an introduction.\n",
    "\n",
    "Unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. However, consider the problem of training a deep network with billions of parameters. How do we achieve this without waiting for days, or even weeks, and thus leaving more time to tune the model? Dean et al. [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) proposed an training paradigm which allows us to train a model on multiple physical machines. The authors describe two methods to achieve this, i.e., **data parallelism** and **model parallelism**<sup>1</sup>.\n",
    "\n",
    "### Model parallelism<sup><span style=\"font-size: 0.75em\">2</span></sup>\n",
    "\n",
    "In model parallelism a *single* model is distributed over multiple machines. The performance benefits of distributing a deep network across multiple machines depends mainly on the structure and of the model. Models with a large number of parameters typically benefit from access to more CPUs and memory, up to the point where communication costs, i.e., propagation of the weight updates and synchronization mechanisms, dominate [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf).\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/model_parallelism.png?raw=true\" alt=\"Model Parallelism\" width=\"300px\" />\n",
    "\n",
    "### Data parallelism\n",
    "\n",
    "As stated in the introduction, in order to train a large network in a reasonable amount of time, we need to parallize the optimization process (which is the learning of the model). In this settings, we take *several* model replicas, and distribute them over multiple machines. Of course, it would also be possible to combine this with the model parallelism approach. However, for the sake of simplicity, let us assume that a model (or several models) can be contained on a single machine. In order to parallelize the training, and to improve the usage of the resources of the cluster, we distribute the models over several machines.\n",
    "\n",
    "In order to build a distributed learning scheme using data parallelism, you would in the most simple case need at least 1 **parameter server**. A parameter server is basically a thread (or a collection of threads) which aggregate the incoming gradient updates of the workers into a so-called center variable, which acts as a *global consensus* variable. Finally, the weights which are stored in the center variable will eventually be used by the produced model.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/data_parallelism.png?raw=true\" alt=\"Data Parallelism\" width=\"400px\" />\n",
    "\n",
    "There are two general approaches towards solving data parallelism. The most straightforward is a **synchronous** method. In short, a synchronous data parallel method will wait for all workers to finish the current mini-batch or stochastic sample before continuing to the next iteration. Synchronous methods have the advantage that all workers will use the most recent center variable, i.e., a worker knows that all other workers will use the same center variable. However, the main disadvantage of this method is the synchronization itself. A synchronous method will never be truly synchronous. This is due to the many, and possibly different, machines. Furthermore, every machine could have a different workload, which would influence the training speed of a worker. As a result, synchronous methods need additional waiting mechanisms to synchronize all workers. These locking mechanisms will make sure that that all workers compute the next gradient based on the same center variable. However, locking mechanisms induce a significant wait which will significantly influence the training speed. For example, imagine a cluster node with an unusual high load. This high load will, due to CPU sharing, cause the training procedure to slow down. Which in turn, will cause the other workers to wait for this single node. Of course, this is just a simple and possibly extreme example, but this example shows how a single worker could significantly influence the training time of all workers.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/synchronous_method.png?raw=true\" alt=\"Data Parallelism\" width=\"500px\" />\n",
    "\n",
    "A very simple, but radical \"solution\" for this synchronization problem is to not synchronize the workers :) Workers simply fetch the center variable and update the parameter server with the computed gradient whenever a worker is ready. This approach is called an **asynchronous** data parallel method. Asynchronous methods, compared to synchronous methods, will have a different set of problems. One of these is a so-called **stale gradient**. This a gradient based on an older version of the center variable while the current center variable is a center variable which has already been updated by other workers. One approach to solve this is to induce and exponential decay factor to the gradient updates. However, this would waste computational resources, but of course, one could just get the most recent weights from the parameter server and then start again. However, as we will show later, it is actually stale gradients (result of asynchrony) that induce *implicit momentum* to the learning process [[2]](https://arxiv.org/pdf/1606.04487v4.pdf).\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/asynchronous_method.png?raw=true\" alt=\"Data Parallelism\" width=\"500px\" />\n",
    "\n",
    "At this point you probably ask the question: **why does this actually work?** A lot of people suggest this is due to the sparsity of the gradients. Intuitively, image having multiple workers processing different data (since every worker has its own dat partition), chances are the weights updates will be totally dissimilar since we are training a large network with a lot of tunable parameters. Furthermore, techniques such as dropout (if they are applied differently among the replicas) only increase the sparsity updates<sup>3</sup>.\n",
    "\n",
    "### Formalization\n",
    "\n",
    "We would also like to inform the reader that the general problem to be solved is the so-called **global consensus optimization** problem. A popular approach towards solving this is using the Alternating Direction Method of Multipliers (ADMM) [[3]](http://www.jmlr.org/proceedings/papers/v32/zhange14.pdf) [[4]](http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf). However, since this is outside the scope of this notebook we will not review this in-depth. But, we would like to note that the *Elastic Averaging* methods [[5]](https://arxiv.org/pdf/1412.6651.pdf) by Zhang et al., which we included in *Distributed Keras* are based on ADMM.\n",
    "\n",
    "<sub>**1:** Hybrids are possible as well.</sub>      \n",
    "<sub>**2:** This is mainly used for the computation of the network outputs [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf).</sub>           \n",
    "<sub>**3:** A way to check the sparsity between 2 gradients is to put all the weights in to a 1 dimensional vector, and then compute the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Keras\n",
    "\n",
    "Distributed Keras is a framework which uses Apache Spark and Keras. We chose for Spark because of the distributed environment. This allows us to preprocess the data in a distributed manner, and train our deep learning models on the same architecture, while still having the modeling simplicity of Keras.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Our architecture is very similar to the architecture discussed in [[1]](https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf). However, we employ Apache Spark for data parallel reading and handling larger than memory datasets. The parameter server will always be created in the **Spark Driver**. This **is the program which creates the Spark Context**. For example, if the Jupyter installation of this notebook is running on the Spark cluster, then a cluster node will host the parameter server. However, if you run a Python script, which connects to a remote Spark cluster, then your computer will run the Spark Driver, and as a result will run the parameter server. In that case, be sure your network connection is able to handle the load, else your computer will be the bottleneck in the learning process.\n",
    "\n",
    "<img src=\"https://github.com/JoeriHermans/dist-keras/blob/master/resources/distkeras_architecture.png?raw=true\" alt=\"Model Parallelism\" width=\"500px\" />\n",
    "\n",
    "### Implementation of costum distributed optimizer\n",
    "\n",
    "In order to implement your own optimizer you need 2 classes. First, define your optimizer using the *Trainer* interface. We already supplied an *AsynchronousDistributedTrainer*, and an *SynchronousDistributedTrainer*. However, if you require an other procedure, please feel free to do so. Finally, you need a worker class. This class must have a *train* method with the required arguments, as specified by Apache Spark.\n",
    "\n",
    "### Usage\n",
    "\n",
    "In the following sections, we will give you an example how a complete workflow will look like. This includes setting up a Spark context, reading, preprocessing, and normalizing the data. Finally, we create a relatively simple model (feel free to adjust the parameters) with Keras and optimize it using the different distributed optimizers which are included by default.\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We are using the ATLAS Higgs dataset constructed for the Kaggle machine learning challenge. This dataset is quite limited, it contains only **250000** instances. 40% of which we will be using as a test set. For future experiments, it would be usefull to integrate well understood datasets such as CIFAR or MNIST to evaluate against other optimizers. However, it would be nice to have a \"well understood\" HEP (High Energy Physics) dataset for this task :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from distkeras.distributed import *\n",
    "from distkeras.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Distributed Keras Notebook\"\n",
    "using_spark_2 = False\n",
    "yarn = \"p01001532067275.cern.ch:8088\" # Address:port of ResourceManager\n",
    "if not yarn:\n",
    "    # Tell master to use local resources.\n",
    "    master = \"local[*]\"\n",
    "    num_cores = 3\n",
    "    num_executors = 1\n",
    "else:\n",
    "    # Tell master to use YARN.\n",
    "    master = \"yarn-client\"\n",
    "    max_num_executors = 6\n",
    "    num_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if YARN is specified.\n",
    "if yarn:\n",
    "    # Build the ResourceManager metrics URI.\n",
    "    yarn_metrics_uri = \"http://\" + yarn + \"/ws/v1/cluster/metrics\"\n",
    "    # Fetch the number of available nodes\n",
    "    response = requests.get(yarn_metrics_uri)\n",
    "    data = response.json()\n",
    "    # Fetch the number of active nodes.\n",
    "    num_active_nodes = int(data['clusterMetrics']['activeNodes'])\n",
    "    # Assign the number of executors.\n",
    "    num_executors = num_active_nodes\n",
    "    if num_executors > max_num_executors:\n",
    "        num_executors = max_num_executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This variable is derived from the number of cores and executors, and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_cores\n",
    "\n",
    "print(\"Number of desired executors: \" + `num_executors`)\n",
    "print(\"Number of desired cores / executor: \" + `num_cores`)\n",
    "print(\"Total number of workers: \" + `num_workers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the DataBricks CSV reader, this has some nice functionality regarding invalid values.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing a Spark Context\n",
    "\n",
    "In order to read our (big) dataset into our Spark Cluster, we first need a Spark Context. However, since Spark 2.0 there are some changes in the initialization of a Spark Context. For example, SQLContext and HiveContext do not have to be initialized seperatly anymore, i.e., the initialization process is simplified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_cores`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n",
    "\n",
    "# Check if the user is running Spark 2.0 +\n",
    "if using_spark_2:\n",
    "    sc = SparkSession.builder.config(conf=conf) \\\n",
    "            .appName(application_name) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    # Create the Spark context.\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Add the missing imports\n",
    "    from pyspark import SQLContext\n",
    "    sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if we are using Spark 2.0\n",
    "if using_spark_2:\n",
    "    reader = sc\n",
    "else:\n",
    "    reader = sqlContext\n",
    "# Read the dataset.\n",
    "raw_dataset = reader.read.format('com.databricks.spark.csv') \\\n",
    "                    .options(header='true', inferSchema='true').load(\"data/atlas_higgs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Double-check the inferred schema, and get fetch a row to show how the dataset looks like.\n",
    "raw_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preprocessing and normalization\n",
    "\n",
    "Since Spark's MLlib has some nice features for some distributed dataprocessing, we follow MLlib (dataframe) API in order to ensure compatibility. What it basically boils down to, is that all the features (which can have different type) will be aggregated into a single column. More information on Spark MLlib (and other APIs) can be found here: [http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "In the following steps we will show you how to extract the desired columns from the dataset and prepare the for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we would like to extract the desired features from the raw dataset.\n",
    "# We do this by constructing a list with all desired columns.\n",
    "features = raw_dataset.columns\n",
    "features.remove('EventId')\n",
    "features.remove('Weight')\n",
    "features.remove('Label')\n",
    "# Next, we use Spark's VectorAssembler to \"assemble\" (create) a vector of all desired features.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#vectorassembler\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "# This transformer will take all columns specified in features, and create an additional column \"features\" which will contain all the desired features aggregated into a single vector.\n",
    "dataset = vector_assembler.transform(raw_dataset)\n",
    "\n",
    "# Show what happened after applying the vector assembler.\n",
    "# Note: \"features\" column got appended to the end.\n",
    "dataset.select(\"features\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply feature normalization with standard scaling. This will transform a feature to have mean 0, and std 1.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#standardscaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_normalized\", withStd=True, withMean=True)\n",
    "standard_scaler_model = standard_scaler.fit(dataset)\n",
    "dataset = standard_scaler_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we look at the dataset, the Label column consists of 2 entries, i.e., b (background), and s (signal).\n",
    "# Our neural network will not be able to handle these characters, so instead, we convert it to an index so we can indicate that output neuron with index 0 is background, and 1 is signal.\n",
    "# http://spark.apache.org/docs/latest/ml-features.html#stringindexer\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"label_index\").fit(dataset)\n",
    "dataset = label_indexer.transform(dataset)\n",
    "\n",
    "# Show the result of the label transformation.\n",
    "dataset.select(\"Label\", \"label_index\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some properties of the neural network for later use.\n",
    "nb_classes = 2 # Number of output classes (signal and background)\n",
    "nb_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We observe that Keras is not able to work with these indexes. What it actually expects is a vector with an identical size to the output layer. Our framework provides functionality to do this with ease. What it basically does, given an expected vector dimension, it prepares zero vector with the specified dimensionality, and will set the neuron with a specific label index to one.\n",
    "\n",
    "# For example:\n",
    "# 1. Assume we have a label index: 3\n",
    "# 2. Output dimensionality: 5\n",
    "# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]\n",
    "\n",
    "label_vector_transformer = LabelVectorTransformer(output_dim=nb_classes, input_col=\"label_index\", output_col=\"label\")\n",
    "dataset = label_vector_transformer.transform(dataset)\n",
    "# Only select the columns we need (less data shuffling) while training.\n",
    "dataset = dataset.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "\n",
    "# Show the expected output vectors of the neural network.\n",
    "dataset.select(\"label_index\", \"label\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: shuffling on a large dataset will take some time.\n",
    "\n",
    "We recommend users to first preprocess and shuffle their data, as is described in the data preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffle the dataset.\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "# Note: we also support shuffling in the trainers by default.\n",
    "# However, since this would require a shuffle for every training we will only do it once here.\n",
    "# If you want, you can enable the training shuffling by specifying shuffle=True in the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, we create a trainingset and a testset.\n",
    "(trainingSet, testSet) = dataset.randomSplit([0.7, 0.3])\n",
    "trainingSet.cache()\n",
    "testSet.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model construction\n",
    "\n",
    "We will now construct a relatively simple Keras model (without any modifications) which, hopefully, will be able to classify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(nb_features,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(500))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.4))\n",
    "#model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarize the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Worker Optimizer and Loss\n",
    "\n",
    "In order to evaluate the gradient on the model replicas, we have to specify an optimizer and a loss method. For this, we just follow the Keras API as defined in the documentation: [https://keras.io/optimizers/](https://keras.io/optimizers/) and [https://keras.io/objectives/](https://keras.io/objectives/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = 'adagrad'\n",
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "In the following cells we will train and evaluate the model using different distributed trainers, however, we will as well provide a baseline metric using a **SingleTrainer**, which is basically an instance of the Adagrad optimizer running on Spark.\n",
    "\n",
    "Furthermore, we will also evaluate every training using Spark's MulticlassClassificationEvaluator [https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluator\n",
    "\n",
    "We will evaluate all algorithms using the F1 [https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_name = \"f1\"\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=metric_name, predictionCol=\"predicted_index\", labelCol=\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    global testSet\n",
    "    \n",
    "    # Clear the prediction column from the testset.\n",
    "    testSet = testSet.select(\"features_normalized\", \"label_index\", \"label\")\n",
    "    # Apply a prediction from a trained model.\n",
    "    predictor = ModelPredictor(keras_model=trained_model, features_col=\"features_normalized\")\n",
    "    testSet = predictor.predict(testSet).toDF()\n",
    "    # Transform the prediction vector to an indexed label.\n",
    "    testSet = index_transformer.transform(testSet).toDF()\n",
    "    # Store the F1 score of the SingleTrainer.\n",
    "    score = evaluator.evaluate(testSet)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we can evaluate the prediction, we will also need to converted the neural network prediction, which is a vector which has the same dimensionality as the output layer, to a label index. This is shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_transformer = LabelIndexTransformer(output_dim=nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to keep track of the evaluated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "time_spent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distribute the training and test set to the workers.\n",
    "testSet = testSet.repartition(num_workers)\n",
    "trainingSet = trainingSet.repartition(num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SingleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "single_trainer = SingleTrainer(keras_model=model, loss=loss, worker_optimizer=optimizer, \n",
    "                               features_col=\"features_normalized\", num_epoch=1, batch_size=64)\n",
    "trained_model = single_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['single_trainer'] = dt\n",
    "\n",
    "# Note that this time also includes shuffling the data from different places. If you run this\n",
    "# for the second time (after each other), it will be a lot faster since the data is already\n",
    "# on the physical machine.\n",
    "print(\"Time spent (SingleTrainer): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['single_trainer'] = score\n",
    "\n",
    "print(\"F1 (SingleTrainer): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "easgd_trainer = EASGD(keras_model=model, loss=loss, worker_optimizer=optimizer, \n",
    "                      features_col=\"features_normalized\", batch_size=64, num_epoch=1,\n",
    "                      num_workers=num_workers, rho=5.0, learning_rate=0.05)\n",
    "trained_model = easgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['easgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (EASGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['easgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (EASGD): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous EASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "async_easgd_trainer = AsynchronousEASGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                        batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                        worker_optimizer=optimizer, loss=loss, communication_window=30)\n",
    "trained_model = async_easgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['async_easgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (Asynchronous EASGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['async_easgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (Asynchronous EASGD): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Asynchronous EAMSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "async_eamsgd_trainer = AsynchronousEAMSGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                          batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                          worker_optimizer=optimizer, loss=loss, communication_window=30, momentum=0.96)\n",
    "trained_model = async_eamsgd_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['async_eamsgd_trainer'] = dt\n",
    "\n",
    "print(\"Time spend (Asynchronous EAMSGD): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['async_eamsgd_trainer'] = score\n",
    "\n",
    "print(\"F1 (Asynchronous EAMSGD): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DOWNPOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "downpour_trainer = DOWNPOUR(keras_model=model, worker_optimizer=optimizer, loss=loss, features_col=\"features_normalized\", batch_size=62,\n",
    "                   num_workers=num_workers, learning_rate=0.05, communication_window=1)\n",
    "trained_model = downpour_trainer.train(trainingSet)\n",
    "dt = time.time() - time_start\n",
    "time_spent['downpour_trainer'] = dt\n",
    "\n",
    "print(\"Time spent (DOWNPOUR): \" + `dt` + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = evaluate(trained_model)\n",
    "results['downpour_trainer'] = score\n",
    "\n",
    "print(\"F1 (DOWNPOUR): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of initial experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the F1 score of the trainers.\n",
    "plt.bar(range(len(results)), results.values(), 0.4, align='center', color='b')\n",
    "plt.xticks(range(len(results)), results.keys(), rotation=25)\n",
    "plt.xlabel('Trainers')\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"F1 score with different trainers - (higher is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training time of the trainers.\n",
    "plt.bar(range(len(time_spent)), time_spent.values(), 0.4, align='center', color='b')\n",
    "plt.xticks(range(len(time_spent)), time_spent.keys(), rotation=25)\n",
    "plt.xlabel('Trainers')\n",
    "plt.ylabel('Seconds')\n",
    "plt.title(\"Training time - (lower is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of parallel workers: \" + `num_workers`)\n",
    "improvement = (time_spent['single_trainer'] / time_spent['async_easgd_trainer']) * 100.0\n",
    "print(\"Improvement using AEASGD: \" + `improvement` + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "In the following section we will investigate the scaling of the algorithms, and how the scaling influences the model performance (statistical efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_num_workers = 2\n",
    "max_num_workers = num_workers\n",
    "scaling_results = {}\n",
    "scaling_time_spent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(min_num_workers, max_num_workers):\n",
    "    print(\"Evaluating for \" + str(i) + \" parallel workers.\")\n",
    "    async_easgd_trainer = AsynchronousEASGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                            batch_size=10, num_workers=i, rho=5.0, learning_rate=0.05,\n",
    "                                            worker_optimizer=optimizer, loss=loss, communication_window=30)\n",
    "    time_start = time.time()\n",
    "    trained_model = async_easgd_trainer.train(trainingSet)\n",
    "    dt = time.time() - time_start\n",
    "    print(\"- Training done. Training time: \" + str(dt))\n",
    "    score = evaluate(trained_model)\n",
    "    print(\"F1 score: \" + str(score))\n",
    "    scaling_results[i] = score\n",
    "    scaling_time_spent[i] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the F1 score of the different number of trainers.\n",
    "x = scaling_results.keys()\n",
    "y = scaling_results.values()\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Number of trainers')\n",
    "plt.ylabel('F1')\n",
    "plt.title(\"Performance vs. parallel trainers - (higher is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below, the **green line** depicts the expected decrease in training time when using $n$ amount of parallel trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training time of the different number of trainers.\n",
    "x_1 = scaling_time_spent.keys()\n",
    "x_2 = np.arange(0, max_num_workers, 1)\n",
    "y_1 = scaling_time_spent.values()\n",
    "y_2 = np.zeros(max_num_workers)\n",
    "y_2.fill(time_spent['single_trainer'])\n",
    "for i in range(0, max_num_workers - 1):\n",
    "    y_2[i] /= i\n",
    "x_2 = list(x_2)\n",
    "y_2 = list(y_2)\n",
    "plt.plot(x_1, y_1)\n",
    "plt.plot(x_2, y_2)\n",
    "plt.xlabel('Number of trainers')\n",
    "plt.ylabel('Wallclock time (seconds)')\n",
    "plt.title(\"Total training time - (lower is better)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchrony induces momentum (research, not finished)\n",
    "\n",
    "**Note:** The following section is subject to research. EASGD derived methods not similar to DOWNPOUR methods, and thus behave different. Furthermore, the theoretical influence of a *communication window* is not very well understood either.\n",
    "\n",
    "As we mentioned in the introduction, for asynchronous methods scaling induces momentum. Meaning, adding more parallel workers adds momentum. This is formalized in the Equation below where $n$ is the number of workers being used, more details are described in [[2]](https://arxiv.org/pdf/1606.04487v4.pdf).\n",
    "\n",
    "$$\\mathbb{E}V_{t+1} = \\Bigg(1 - \\frac{1}{n}\\Bigg)\\mathbb{E}V_t - \\frac{\\eta}{n}\\mathbb{E}\\nabla f(x)$$\n",
    "\n",
    "Using the right value can significantly reduce the number of iterations to reach the desired target loss. As seen from the plots the statistical performance of asynchronous EASGD reduces significantly. Note that this implementation does not have any explicit momentum term. This means that the algorithm is equivalent to EAMSGD, but with momentum 0. In order to confirm this statement, can we tune momentum in such a way (given the theory) that we will achieve a good model performance while using more parallel workers, and thus reducing training time.\n",
    "\n",
    "First, we will run EAMSGD with momentum 0.0 in order to demonstrate the equivalence with asynchronous EASGD. Note that both will have implicit momentum due to the asynchronous method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the implicit momentum induced by the asynchronous method.\n",
    "implicit_momentum = (1 - 1.0 / num_workers)\n",
    "\n",
    "print(\"Implicit momentum: \" + str(implicit_momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async_easgd_trainer = AsynchronousEASGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                        batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                        worker_optimizer=optimizer, loss=loss, communication_window=30)\n",
    "trained_model = async_easgd_trainer.train(trainingSet)\n",
    "score = evaluate(trained_model)\n",
    "\n",
    "print(\"F1 (Asynchronous EASGD) with implicit momentum: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async_eamsgd_trainer = AsynchronousEAMSGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                          batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                          worker_optimizer=optimizer, loss=loss, communication_window=30,\n",
    "                                          momentum=0.0)\n",
    "trained_model = async_eamsgd_trainer.train(trainingSet)\n",
    "score = evaluate(trained_model)\n",
    "\n",
    "print(\"F1 (EAMSGD with expl. momentum 0.0 and impl. momentum): \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tune the momentum turn in such a way (according to the theory) we will achieve the same statistical performance with a smaller of number of parallel workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We estimated (guessing) the optimal momentum of the dataset.\n",
    "optimal_momentum = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrected_momentum = (optimal_momentum - implicit_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Our *explicit* momentum is ***negative*** in order to compensate for the implicit momentum induced by the asynchronous method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "async_eamsgd_trainer = AsynchronousEAMSGD(keras_model=model, features_col=\"features_normalized\",\n",
    "                                          batch_size=10, num_workers=num_workers, rho=5.0, learning_rate=0.05,\n",
    "                                          worker_optimizer=optimizer, loss=loss, communication_window=30,\n",
    "                                          momentum=corrected_momentum)\n",
    "trained_model = async_eamsgd_trainer.train(trainingSet)\n",
    "score = evaluate(trained_model)\n",
    "\n",
    "print(\"F1 (Asynchronous EAMSGD with momentum 0.0): \" + `score`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Personal note**: It seems that a large communication window reduces the implicit momentum. However, this has to be examined further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental observations\n",
    "\n",
    "- DOWNPOUR converges well when a small communication window is used $< 5$.\n",
    "- EASGD based methods on the other hand, thrive using large communication windows $> 25$.\n",
    "- Asynchronous methods induce implicit momentum.\n",
    "- Smaller ($< 64$) mini-batch sizes improve EASGD performance, but for the synchronous case this increases synchronization time.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Distributed Deep Learning can **significantly speedup** the learning process. We provide such a framework built on top of Keras and Apache Spark. The latter provides a nice framework for distributed data processing and model evaluation. We can easily integrate our workflows using Apache Spark, and thus speeding up the **data preprocessing**, and our **model optimization procedure** while still having the same **modelling simplicity**.\n",
    "\n",
    "Our group is always open on further collaboration on this work, and would like to assist the physics community in their machine learning efforts.\n",
    "\n",
    "**Contact**: [joeri.hermans@cern.ch](mailto:joeri.hermans@cern.ch)\n",
    "             [luca.canali@cern.ch](mailto:luca.canali@cern.ch)\n",
    "             [zbigniew.baranowski@cern.ch](mailto:zbigniew.baranowski@cern.ch)\n",
    "\n",
    "## Future work\n",
    "\n",
    "- Understanding \"theoretical\" meaning of a communication window.\n",
    "- Apply compression for big weight updates when sending updates to the parameter server.\n",
    "- Keep track of a gradient residual (this will reduce the bandwidth due to sparsity).\n",
    "- Evaluation of algorithms with GPU's.\n",
    "- Optimization of parameter sharing (e.g., sockets instead of REST API).\n",
    "- Use \"famous\" ConvNet architectures with well known datasets in order to have a more sound evaluation.\n",
    "- Add threaded queue to process asynchronous updates.\n",
    "- Training accuracy while training.\n",
    "- Stop on target loss.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Many thanks to Zbigniew Baranowski and Luca Canali of the IT-DB group, and to Jean-Roch Vlimant, Maurizio Pierini, and Federico Presutti of the EP-UCM group for their collaboration on this work.\n",
    "\n",
    "## GitHub repository\n",
    "\n",
    "[https://github.com/JoeriHermans/dist-keras/](#https://github.com/JoeriHermans/dist-keras)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
